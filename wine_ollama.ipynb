{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6806af9-68ae-4714-851c-9a967aee0e23",
   "metadata": {},
   "source": [
    "# Leveraging an OpenAI cookbook to make some tests\n",
    "\n",
    "In October 2024 OpenAI released a cookbook to show how to do model distillation and used a Wine based scenario that look an interesting test for various models against each other. \n",
    "In this cookbook we'll not do any distillation or fine-tuning (yet, maybe in a future article), bet we'll look at a dataset and see how various models perform on it.\n",
    "\n",
    "We'll also leverage **Structured Outputs** for a classification problem using a list of enum. We'll show that **Structured Ouputs** work with all of those models, and we'll use plain json for response format and Ollama Python client too.\n",
    "\n",
    "We'll first analyze the dataset and get the output of llama3.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd8fd2f-dfdf-47c2-9627-02acbe3fb7a2",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Let's install and load dependencies.\n",
    "Make sure your API keys are defined in your .env file as \"OPENAI_API_KEY\", \"GEMINI_API_KEY\" or \"OPENROUTER_API_KEY\" and be'll be loaded by scripts directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e16ed9ef-0220-4f23-a8eb-40813eacf210",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install ollama numpy pandas tqdm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b643798-3b2b-43e4-bfb5-ebcf74066253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246364b6-2fed-4b54-b540-09569a197a6b",
   "metadata": {},
   "source": [
    "## Loading and understanding the dataset\n",
    "\n",
    "For this cookbook, we'll load the data from the following Kaggle challenge: [https://www.kaggle.com/datasets/zynicide/wine-reviews](https://www.kaggle.com/datasets/zynicide/wine-reviews). You have to download it and save it in a data folder in the same level of this notebook.\n",
    "\n",
    "This dataset has a large number of rows and you're free to run this cookbook on the whole data, but as a biaised italian wine-lover, I'll narrow down the dataset to only Italian wine to focus on less rows and grape varieties. The original article was on French wine and tested LLMs are better in guessing them. More on results later. I made the variable generic so that you can change country and test with wine you know/love (keep Italian in this case ðŸ˜‚)\n",
    "\n",
    "We're looking at a classification problem where we'd like to guess the grape variety based on all other criterias available, including description, subregion and province that we'll include in the prompt. It gives a lot of information to the model, you're free to also remove some information that can help significantly the model such as the region in which it was produced to see if it does a good job at finding the grape.\n",
    "\n",
    "Let's filter the grape varieties that have less than 5 occurences in reviews.\n",
    "\n",
    "Let's proceed with a subset of 500 random rows from this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "759d1705-2213-443a-9fc3-050bc00177e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>designation</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>region_2</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>taster_twitter_handle</th>\n",
       "      <th>title</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42286</th>\n",
       "      <td>42286</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Lean and linear, this offers white spring flow...</td>\n",
       "      <td>Fumat</td>\n",
       "      <td>86</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Northeastern Italy</td>\n",
       "      <td>Collio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kerin Oâ€™Keefe</td>\n",
       "      <td>@kerinokeefe</td>\n",
       "      <td>Collavini 2015 Fumat Sauvignon (Collio)</td>\n",
       "      <td>Sauvignon</td>\n",
       "      <td>Collavini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82695</th>\n",
       "      <td>82695</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Here's a lush and modern Rosso di Montalcino w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Tuscany</td>\n",
       "      <td>Rosso di Montalcino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tenute Silvio Nardi 2008  Rosso di Montalcino</td>\n",
       "      <td>Sangiovese Grosso</td>\n",
       "      <td>Tenute Silvio Nardi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94283</th>\n",
       "      <td>94283</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Made with 60% Sangiovese, 25% Cabernet Sauvign...</td>\n",
       "      <td>Casal Duro</td>\n",
       "      <td>88</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Tuscany</td>\n",
       "      <td>Toscana</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kerin Oâ€™Keefe</td>\n",
       "      <td>@kerinokeefe</td>\n",
       "      <td>Fattoria La Vialla 2012 Casal Duro Red (Toscana)</td>\n",
       "      <td>Red Blend</td>\n",
       "      <td>Fattoria La Vialla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62052</th>\n",
       "      <td>62052</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Here's a harmonious and well-balanced Barbera ...</td>\n",
       "      <td>Molisse</td>\n",
       "      <td>89</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Piedmont</td>\n",
       "      <td>Barbera d'Asti Superiore</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Agostino Pavia &amp; Figli 2007 Molisse  (Barbera ...</td>\n",
       "      <td>Barbera</td>\n",
       "      <td>Agostino Pavia &amp; Figli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99042</th>\n",
       "      <td>99042</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Subdued aromas of red berry, dried meat and a ...</td>\n",
       "      <td>Moganazzi Volta Sciara Rosso</td>\n",
       "      <td>87</td>\n",
       "      <td>45.0</td>\n",
       "      <td>Sicily &amp; Sardinia</td>\n",
       "      <td>Etna</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kerin Oâ€™Keefe</td>\n",
       "      <td>@kerinokeefe</td>\n",
       "      <td>Le Vigne di Eli 2012 Moganazzi Volta Sciara Ro...</td>\n",
       "      <td>Red Blend</td>\n",
       "      <td>Le Vigne di Eli</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 country                                        description  \\\n",
       "42286       42286   Italy  Lean and linear, this offers white spring flow...   \n",
       "82695       82695   Italy  Here's a lush and modern Rosso di Montalcino w...   \n",
       "94283       94283   Italy  Made with 60% Sangiovese, 25% Cabernet Sauvign...   \n",
       "62052       62052   Italy  Here's a harmonious and well-balanced Barbera ...   \n",
       "99042       99042   Italy  Subdued aromas of red berry, dried meat and a ...   \n",
       "\n",
       "                        designation  points  price            province  \\\n",
       "42286                         Fumat      86   19.0  Northeastern Italy   \n",
       "82695                           NaN      88   25.0             Tuscany   \n",
       "94283                    Casal Duro      88   28.0             Tuscany   \n",
       "62052                       Molisse      89   19.0            Piedmont   \n",
       "99042  Moganazzi Volta Sciara Rosso      87   45.0   Sicily & Sardinia   \n",
       "\n",
       "                       region_1 region_2    taster_name taster_twitter_handle  \\\n",
       "42286                    Collio      NaN  Kerin Oâ€™Keefe          @kerinokeefe   \n",
       "82695       Rosso di Montalcino      NaN            NaN                   NaN   \n",
       "94283                   Toscana      NaN  Kerin Oâ€™Keefe          @kerinokeefe   \n",
       "62052  Barbera d'Asti Superiore      NaN            NaN                   NaN   \n",
       "99042                      Etna      NaN  Kerin Oâ€™Keefe          @kerinokeefe   \n",
       "\n",
       "                                                   title            variety  \\\n",
       "42286            Collavini 2015 Fumat Sauvignon (Collio)          Sauvignon   \n",
       "82695      Tenute Silvio Nardi 2008  Rosso di Montalcino  Sangiovese Grosso   \n",
       "94283   Fattoria La Vialla 2012 Casal Duro Red (Toscana)          Red Blend   \n",
       "62052  Agostino Pavia & Figli 2007 Molisse  (Barbera ...            Barbera   \n",
       "99042  Le Vigne di Eli 2012 Moganazzi Volta Sciara Ro...          Red Blend   \n",
       "\n",
       "                       winery  \n",
       "42286               Collavini  \n",
       "82695     Tenute Silvio Nardi  \n",
       "94283      Fattoria La Vialla  \n",
       "62052  Agostino Pavia & Figli  \n",
       "99042         Le Vigne di Eli  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/winemag-data-130k-v2.csv')\n",
    "df_country = df[df['country'] == 'Italy']\n",
    "\n",
    "# Let's also filter out wines that have less than 5 references with their grape variety â€“ even though we'd like to find those\n",
    "# they're outliers that we don't want to optimize for that would make our enum list be too long\n",
    "# and they could also add noise for the rest of the dataset on which we'd like to guess, eventually reducing our accuracy.\n",
    "\n",
    "varieties_less_than_five_list = df_country['variety'].value_counts()[df_country['variety'].value_counts() < 5].index.tolist()\n",
    "df_country = df_country[~df_country['variety'].isin(varieties_less_than_five_list)]\n",
    "\n",
    "df_country_subset = df_country.sample(n=500)\n",
    "df_country_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96cd12f-cbdf-46af-958f-3d553598be1d",
   "metadata": {},
   "source": [
    "Let's retrieve all grape varieties to include them in the prompt and in our structured outputs enum list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06f5dbea-549a-455d-9b6e-051de9d38723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['White Blend', 'Frappato', 'Nerello Mascalese', \"Nero d'Avola\",\n",
       "       'Red Blend', 'Cabernet Sauvignon', 'Primitivo', 'Catarratto',\n",
       "       'Inzolia', 'Grillo', 'Sangiovese', 'Aglianico', 'Vernaccia',\n",
       "       'Rosato', 'Vermentino', 'Nebbiolo', 'Barbera', 'Sauvignon',\n",
       "       'Sangiovese Grosso', 'Prugnolo Gentile', 'Pinot Bianco',\n",
       "       'Montepulciano', 'Moscato', 'Friulano', 'Sagrantino', 'Prosecco',\n",
       "       'Garganega', 'Chardonnay', 'Sauvignon Blanc', 'Pinot Grigio',\n",
       "       'GewÃ¼rztraminer', 'Cortese', 'Sparkling Blend', 'Cannonau',\n",
       "       'Kerner', 'Dolcetto', 'Glera', 'Syrah', 'Pinot Nero', 'Verduzzo',\n",
       "       'Verdicchio', 'Carricante', 'Fiano', 'Greco', 'Trebbiano', 'RosÃ©',\n",
       "       'Pinot Noir', 'Corvina, Rondinella, Molinara', 'Insolia',\n",
       "       'Ribolla Gialla', 'PriÃ© Blanc', 'Zibibbo', 'Falanghina',\n",
       "       'Negroamaro', 'MÃ¼ller-Thurgau', 'Teroldego', 'Merlot', 'Turbiana',\n",
       "       'Refosco', 'Manzoni', 'RuchÃ©', 'Nero di Troia',\n",
       "       'Lambrusco di Sorbara', 'Lagrein', 'Tocai', 'Pecorino', 'Arneis',\n",
       "       'Nosiola', 'Perricone', 'Albana', 'Lambrusco', 'Grechetto',\n",
       "       'Carignano', 'Shiraz', 'Pallagrello', 'Viognier', 'Aleatico',\n",
       "       'Nascetta', 'Lambrusco Grasparossa', 'Schiava', 'Corvina',\n",
       "       'Moscadello', 'Durella', 'Malvasia', 'Passerina', 'Uva di Troia',\n",
       "       'Cabernet Franc', 'Raboso', 'Champagne Blend',\n",
       "       'Trebbiano Spoletino', 'Riesling', 'Brachetto', 'Mantonico',\n",
       "       'Pallagrello Bianco', 'Sylvaner', 'Picolit', 'Gaglioppo',\n",
       "       'Piedirosso', 'Susumaniello', 'Nerello Cappuccio',\n",
       "       'GrÃ¼ner Veltliner', \"Cesanese d'Affile\", 'Moscato Giallo',\n",
       "       'Cabernet', 'Coda di Volpe', 'Timorasso', 'Petit Verdot',\n",
       "       'Greco Bianco', 'Tocai Friulano', 'Alicante', 'Grecanico',\n",
       "       'Casavecchia', 'Pallagrello Nero', 'Traminer', 'Malvasia Nera',\n",
       "       'Verdeca', 'Tempranillo'], dtype='<U29')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "varieties = np.array(df_country['variety'].unique()).astype('str')\n",
    "varieties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6612e612-698d-4f1a-8008-70fb5ff263a0",
   "metadata": {},
   "source": [
    "## Generating the prompt\n",
    "\n",
    "Let's build out a function to generate our prompt and try it for the first wine of our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ec2fba-9c99-4cb7-bf56-f13e3816e559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(row, varieties):\n",
    "    # Format the varieties list as a comma-separated string\n",
    "    variety_list = ', '.join(varieties)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Based on this wine review, guess the grape variety:\n",
    "    This wine is produced by {row['winery']} in the {row['province']} region of {row['country']}.\n",
    "    It was grown in {row['region_1']}. It is described as: \"{row['description']}\".\n",
    "    The wine has been reviewed by {row['taster_name']} and received {row['points']} points.\n",
    "    The price is {row['price']}.\n",
    "\n",
    "    Here is a list of possible grape varieties to choose from: {variety_list}.\n",
    "    \n",
    "    What is the likely grape variety? Answer only with the grape variety name or blend from the list.\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "# Example usage with a specific row\n",
    "prompt = generate_prompt(df_country_subset.iloc[0], varieties)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62bc556-3e99-4eb9-9db1-50a5a8935314",
   "metadata": {},
   "source": [
    "Here we use Ollama Python client library to call models and we relies on its way of managing [Structured Output](https://ollama.com/blog/structured-outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85cb4cc7-077a-4afe-aefc-85b768adfc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WineVariety(BaseModel):\n",
    "    variety: str = Field(enum=varieties.tolist())\n",
    "\n",
    "# Function to call the API and process the result for a single model (blocking call in this case)\n",
    "def call_model(model, prompt):\n",
    "    response = chat(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You're a sommelier expert and you know everything about wine. You answer precisely with the name of the variety/blend.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        format=WineVariety.model_json_schema()\n",
    "    )\n",
    "    wine_variety = WineVariety.model_validate_json(response.message.content)\n",
    "    return wine_variety.variety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4ae8e1-20cb-43b4-a7b1-9998e09f4394",
   "metadata": {},
   "source": [
    "## Processing\n",
    "\n",
    "As we'll run this locally using Ollama on a single machine, I have removed the code for parallelism of the original article for Ollama and this notebook, but it's still available in other files for OpenAI, Gemini and OpenRouter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81ddec71-c2a6-411a-aecd-6556cddc36c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(index, row, model, df, progress_bar):\n",
    "    global progress_index\n",
    "\n",
    "    try:\n",
    "        # Generate the prompt using the row\n",
    "        prompt = generate_prompt(row, varieties)\n",
    "\n",
    "        df.at[index, model + \"-variety\"] = call_model(model, prompt)\n",
    "        \n",
    "        # Update the progress bar\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        progress_index += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing model {model}: {str(e)}\")\n",
    "\n",
    "def process_dataframe(df, model):\n",
    "    global progress_index\n",
    "    progress_index = 1  # Reset progress index\n",
    "\n",
    "    # Create a tqdm progress bar\n",
    "    with tqdm(total=len(df), desc=\"Processing rows\") as progress_bar:\n",
    "        # Process each example sequentially\n",
    "        for index, row in df.iterrows():\n",
    "            try:\n",
    "                process_example(index, row, model, df, progress_bar)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing example: {str(e)}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680587d1-bcb8-45bd-bfe8-1f771ae1a5bd",
   "metadata": {},
   "source": [
    "Let's try out our call model function before processing the whole dataframe and check the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95915fd2-24f6-4908-a54f-9ce59073233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = call_model('llama3.2', generate_prompt(df_country_subset.iloc[0], varieties))\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941b9027-eb22-4eed-84de-8b962fe4fee2",
   "metadata": {},
   "source": [
    "Great! We confirmed we can get a grape variety as an output, let's now process the dataset with both `llama3.2` and `llama3.1` and compare the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cedf978-e60b-4f56-9c8c-da6b0a722320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [06:48<00:00,  1.22it/s]\n"
     ]
    }
   ],
   "source": [
    "df_country_subset = process_dataframe(df_country_subset, \"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93a96539-e25d-4d15-adc0-ef8cbacd70e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [12:02<00:00,  1.45s/it]\n"
     ]
    }
   ],
   "source": [
    "df_country_subset = process_dataframe(df_country_subset, \"llama3.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74869ee-3361-4d08-bd46-126239d6008c",
   "metadata": {},
   "source": [
    "## Comparing llama3.2 and llama3.1\n",
    "\n",
    "Now that we've got all chat completions for those two models ; let's compare them against the expected grape variety and assess their accuracy at finding it. We'll do this directly in python here as we've got a simple string check to run, but if your task involves more complex evals you can leverage OpenAI Evals or our open-source eval framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cce147d2-32e4-4005-9f79-11394c76c2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama3.2 accuracy: 49.20%\n",
      "llama3.1 accuracy: 61.60%\n"
     ]
    }
   ],
   "source": [
    "models = ['llama3.2', 'llama3.1']\n",
    "\n",
    "def get_accuracy(model, df):\n",
    "    return np.mean(df['variety'] == df[model + '-variety'])\n",
    "\n",
    "for model in models:\n",
    "    print(f\"{model} accuracy: {get_accuracy(model, df_country_subset) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6ae1c1-22e5-4a61-a055-7b98360de523",
   "metadata": {},
   "source": [
    "We can see that llama3.1 is better a finding grape variety than llama3.2 (12.40% higher!). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78a1b24e-3956-48f0-ae56-399f729a6c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetuned model: ft:gpt-4o-mini-2024-07-18:distillation-test:wine-distillation:AIZntSyE\n"
     ]
    }
   ],
   "source": [
    "# copy paste your fine-tune job ID below\n",
    "finetune_job = client.fine_tuning.jobs.retrieve(\"ftjob-pRyNWzUItmHpxmJ1TX7FOaWe\")\n",
    "\n",
    "if finetune_job.status == 'succeeded':\n",
    "    fine_tuned_model = finetune_job.fine_tuned_model\n",
    "    print('finetuned model: ' + fine_tuned_model)\n",
    "else:\n",
    "    print('finetuned job status: ' + finetune_job.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cad42199-a4cb-4589-859b-32b39ad02eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o accuracy: 79.67%\n",
      "gpt-4o-mini accuracy: 64.67%\n",
      "ft:gpt-4o-mini-2024-07-18:distillation-test:wine-distillation:AIZntSyE accuracy: 79.33%\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(f\"{model} accuracy: {get_accuracy(model, another_subset) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5bdb3c-c7ee-493a-9343-5950d7c36638",
   "metadata": {},
   "source": [
    "This notebook was just an introduction and a small playground.\n",
    "In the repository you can find additional python files to try, in all of them Structured Output has been used:\n",
    "- wine_anthropic.py: Anthropic using native Anthropic API\n",
    "- wine_deepseek.py: DeepSeek V3 using native DeepSeek API with 500(!!!) parallel threads for unbelievably fast results\n",
    "- wine_gemini.py: gemini models using native Gemini API\n",
    "- wine_gemini_openai.py: gemini models using OpenAI API\n",
    "- wine_lmstudio.py: I used this for Apple MLX models through LMStudio, but you can test any models loadable by LMStudio\n",
    "- wine_ollama.py: code similar to this notebook, but you can pass multiple models and let it run to test them all\n",
    "- wine_openai: for OpenAI models using native OpenAI API\n",
    "- wine_openrouter:\n",
    "  - here you can test any model available on OpenRouter. Just be aware that Structured Output and response are not managed in the same way by all LLMs and you can get errors. In this case you need to tweak the call_model function.\n",
    "  - The model used in the example is DeepSeek V3 with 200 (Yes!!!) parallel thread. \n",
    "\n",
    "**Have fun!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd141b02-3246-4a6e-8e61-85109eeca038",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
